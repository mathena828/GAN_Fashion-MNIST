{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CGAN-MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPNG9IzIXUsjaDvrc2fJSb4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ww3ll_uNY4_3",
        "outputId": "5d830b37-cb67-4c8b-8259-92327759e197"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfsLcMo4TcLS"
      },
      "source": [
        "# Import libraries.\n",
        "import os\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot\n",
        "from keras.optimizers import Adam\n",
        "from numpy.random import randn, randint\n",
        "from keras.models import Model, load_model\n",
        "from numpy import expand_dims, ones, zeros, asarray\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Conv2D, Conv2DTranspose, Dropout, Embedding, Concatenate, LeakyReLU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHOtmxSYJUd2"
      },
      "source": [
        "!pip install -q imageio\n",
        "!pip install -q git+https://github.com/tensorflow/docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9jd0mWfnYqa"
      },
      "source": [
        "# Download the Fashion MNIST dataset.\n",
        "(train_images, train_labels), (test_images, test_labels) =  tf.keras.datasets.fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlSBzDLx10uq"
      },
      "source": [
        "# Pre-process the 60,000 training set images.\n",
        "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32') # Convert pixels to floating point values. \n",
        "train_images = (train_images - 127.5) / 127.5 # Normalize the pixel values to be in the range of [-1,1].\n",
        "train_dataset = [train_images, train_labels]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGR-PobSWhHa"
      },
      "source": [
        "# Perform hyperparameter optimization.\n",
        "BATCH = 256\n",
        "EPOCHS = 10 \n",
        "LATENT_DIM = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt4SsAwU5vts"
      },
      "source": [
        "# Define the generator.\n",
        "def build_generator(latent_dim, classes=10):\n",
        "  # Each of the 10 classes for the Fashion MNIST dataset will map to a different 50-element vector representation.\n",
        "  labels = Input(shape=(1,))\n",
        "  input_labels = Embedding(classes, 50)(labels)\n",
        "  # The embedding is passed through a fully connected layer with a linear activation.\n",
        "  nodes = 7 * 7\n",
        "  input_labels = Dense(nodes)(input_labels)\n",
        "  input_labels = Reshape((7, 7, 1))(input_labels) # Reshape into (None, 7, 7, 1).\n",
        "  # Initialize the image generator input.\n",
        "  input_latent = Input(shape=(latent_dim,))\n",
        "  nodes = 128 * 7 * 7\n",
        "  layer = Dense(nodes)(input_latent)\n",
        "  layer = LeakyReLU(alpha=0.2)(layer)\n",
        "  layer = Reshape((7, 7, 128))(layer)\n",
        "  merge = Concatenate()([layer, input_labels]) # Concatenate the input labels and with the input images.\n",
        "  # Upsample with Conv2D and LeakyReLU.\n",
        "  layer = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(merge)\n",
        "  layer = LeakyReLU(alpha=0.2)(layer)\n",
        "  layer = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(layer)\n",
        "  layer = LeakyReLU(alpha=0.2)(layer)\n",
        "  output_layer = Conv2D(1, (7,7), activation='tanh', padding='same')(layer)\n",
        "  model = Model([input_latent, labels], output_layer) # Define the model.\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8x_O67kXNgk"
      },
      "source": [
        "# Build the generator.\n",
        "generator = build_generator(LATENT_DIM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZI_A29U5s5O"
      },
      "source": [
        "# Define the discriminator.\n",
        "def build_discriminator(input_shape=(28,28,1), classes=10):\n",
        "  # Each of the 10 classes for the Fashion MNIST dataset will map to a different 50-element vector representation that will be learned by the discriminator model.\n",
        "\tlabels = Input(shape=(1,))\n",
        "\tinput_labels = Embedding(classes, 50)(labels)\n",
        "  # The embedding is scaled up to image dimensions with linear activation.\n",
        "\tnodes = input_shape[0] * input_shape[1]\n",
        "\tinput_labels = Dense(nodes)(input_labels)\n",
        "\tinput_labels = Reshape((input_shape[0], input_shape[1], 1))(input_labels) # Reshape into (None, 28, 28, 1).\n",
        "\tinput_images = Input(shape=input_shape)\n",
        "\tmerge = Concatenate()([input_images, input_labels]) # Concatenate the input labels and with the input images.\n",
        "\t# Downsample with Conv2D and LeakyReLU.\n",
        "\tlayer = Conv2D(128, (3,3), strides=(2,2), padding='same')(merge)\n",
        "\tlayer = LeakyReLU(alpha=0.2)(layer)\n",
        "\tlayer = Conv2D(128, (3,3), strides=(2,2), padding='same')(layer)\n",
        "\tlayer = LeakyReLU(alpha=0.2)(layer)\n",
        "\t# Flatten feature maps and pass it to the dense layer to complete the classifier.\n",
        "\tlayer = Flatten()(layer)\n",
        "\tlayer = Dropout(0.4)(layer)\n",
        "\toutput_layer = Dense(1, activation='sigmoid')(layer)\n",
        "\t# Define and compile the model with an Adam optimizer.\n",
        "\tmodel = Model([input_images, labels], output_layer)\n",
        "\tadam = Adam(lr=0.0002, beta_1=0.5)\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1z7IytfqW0QJ"
      },
      "source": [
        "# Build the discriminator.\n",
        "discriminator = build_discriminator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5qmEIWqEu_q"
      },
      "source": [
        "# Define the CGAN.\n",
        "def build_cgan(generator, discriminator):\n",
        "\tdiscriminator.trainable = False # Set the weights in the discriminator to be not trainable.\n",
        "\t# Get the noise, label inputs, and image outputs from generator.\n",
        "\tnoise, label = generator.input\n",
        "\toutput = generator.output\n",
        "\t# Connect the image output and label input from the generator as inputs to the discriminator.\n",
        "\tcgan_output = discriminator([output, label])\n",
        "\tmodel = Model([noise, label], cgan_output)\n",
        "\t# Compile the model with an Adam optimizer.\n",
        "\tadam = Adam(lr=0.0002, beta_1=0.5)\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=adam)\n",
        "\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miXZo3ISXdC1"
      },
      "source": [
        "# Build the CGAN.\n",
        "cgan = build_cgan(generator, discriminator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ-Im2opGsRI"
      },
      "source": [
        "# Select real samples from the dataset.\n",
        "def get_real_samples(train_dataset, samples):\n",
        "  images, labels = train_dataset\n",
        "  index = randint(0, images.shape[0], samples)\n",
        "  x, labels = images[index], labels[index]\n",
        "  y = ones((samples, 1)) # Generate class labels of value 1 to indicate the images are real.\n",
        "  return [x, labels], y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNCmEAtUH5z-"
      },
      "source": [
        "# Create an array of randomly selected integer class labels for randomly selected points in the latent space.\n",
        "def get_latent_points(latent_dim, samples, classes=10):\n",
        "\tx_input = randn(latent_dim * samples)\n",
        "\tz_input = x_input.reshape(samples, latent_dim)\n",
        "\tlabels = randint(0, classes, samples)\n",
        "\treturn [z_input, labels]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZheYfjDKBOd"
      },
      "source": [
        "# Generate fake images with class labels using the generator.\n",
        "def get_fake_samples(generator, latent_dim, samples):\n",
        "\tz_input, labels_input = get_latent_points(latent_dim, samples)\n",
        "\timages = generator.predict([z_input, labels_input])\n",
        "\ty = zeros((samples, 1)) # Generate class labels of value 0 to indicate the images are fake.\n",
        "\treturn [images, labels_input], y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4adAeubCwxam"
      },
      "source": [
        "# Save checkpoints during training so the model can be restored.\n",
        "checkpoint_dir = '/content/gdrive/My Drive/Fashion Synthesis/CGAN'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"checkpoint\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=Adam(lr=0.0002, beta_1=0.5), discriminator_optimizer=Adam(lr=0.0002, beta_1=0.5), generator=generator, discriminator=discriminator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVOdm4XxDkyc"
      },
      "source": [
        "# Create and save generated images.\n",
        "def save_plot(examples, n, epoch):\n",
        "  fig = pyplot.figure(figsize=(10,10))\n",
        "  for i in range(n * n):\n",
        "    pyplot.subplot(n, n, 1 + i)\n",
        "    pyplot.imshow(examples[i, :, :, 0], cmap='gray')\n",
        "    pyplot.axis('off')\n",
        "  pyplot.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  pyplot.show()\n",
        "  \n",
        "\n",
        "# Periodically use the generator to generate a sample of images.\n",
        "def summarize_performance(generator, epoch, samples=100):\n",
        "  model = generator\n",
        "  latent_points, labels = get_latent_points(100, 100)\n",
        "  labels = asarray([x for _ in range(10) for x in range(10)])\n",
        "  final  = model.predict([latent_points, labels])\n",
        "  final = (final + 1) / 2.0 # Scale from [-1,1] to [0,1]\n",
        "  save_plot(final, 10, epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJm-B71sKGC4"
      },
      "source": [
        "# Iterate over the number of epochs and train the CGAN on the dataset in batches.\n",
        "def train(generator, discriminator, cgan, train_dataset, latent_dim, epochs, batch):\n",
        "  batch_per_epoch = int(train_dataset[0].shape[0] / batch)\n",
        "  half_batch = int(batch / 2)\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time() # Get the starting time of each epoch.\n",
        "    for batch in range(batch_per_epoch):\n",
        "      # Update discriminator model weights using randomly selected real images.\n",
        "      [images_real, labels_real], y_real = get_real_samples(train_dataset, half_batch)\n",
        "      d_loss1, _ = discriminator.train_on_batch([images_real, labels_real], y_real)\n",
        "      # Update discriminator model weights using fake images.\n",
        "      [images_fake, labels_fake], y_fake = get_fake_samples(generator, latent_dim, half_batch)\n",
        "      d_loss2, _ = discriminator.train_on_batch([images_fake, labels_fake], y_fake)\n",
        "      # Update the generator based on the discriminator's error.\n",
        "      [z_input, labels_input] = get_latent_points(latent_dim, batch)\n",
        "      y_cgan = ones((batch, 1)) \n",
        "      cgan_loss = cgan.train_on_batch([z_input, labels_input], y_cgan)\n",
        "      # print('>%d, %d/%d, discriminator loss: [%.3f,%.3f] generator loss: [%.3f]' % (epoch+1, batch+1, batch_per_epoch, d_loss1, d_loss2, cgan_loss))\n",
        "    summarize_performance(generator, epoch)\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "    print('Time for epoch {} is {} seconds.'.format(epoch + 1, time.time()-start)) # Record how long it took for each epoch to run.\n",
        "  # # Save the generator.\n",
        "  # generator.save('cgan_generator.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjWL9-1ZL_RL"
      },
      "source": [
        "train(generator, discriminator, cgan, train_dataset, LATENT_DIM, EPOCHS, BATCH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBDKMjhTxDxc"
      },
      "source": [
        "# Restore training from the latest saved checkpoint.\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqioTag7wg3a"
      },
      "source": [
        "# Create and save generated images.\n",
        "def save_plot(examples, n):\n",
        "\tfor i in range(n * n):\n",
        "\t\tpyplot.subplot(n, n, 1 + i)\n",
        "\t\tpyplot.axis('off')\n",
        "\t\tpyplot.imshow(examples[i, :, :, 0], cmap='gray')\n",
        "\tpyplot.show()\n",
        " \n",
        "# Load the trained model.\n",
        "model = load_model('cgan_generator.h5', compile=False)\n",
        "latent_points, labels = get_latent_points(100, 100)\n",
        "labels = asarray([x for _ in range(10) for x in range(10)])\n",
        "final  = model.predict([latent_points, labels])\n",
        "final = (final + 1) / 2.0 # Scale from [-1,1] to [0,1]\n",
        "save_plot(final, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEQqM55KRfsh"
      },
      "source": [
        "# Create a gif using all the saved images.\n",
        "gif_file = 'cgan_fashion_mnist.gif'\n",
        "with imageio.get_writer(gif_file, mode='I') as writer:\n",
        "  filenames = glob.glob('image*.png')\n",
        "  filenames = sorted(filenames)\n",
        "  for filename in filenames:\n",
        "    image = imageio.imread(filename)\n",
        "    writer.append_data(image)\n",
        "  image = imageio.imread(filename)\n",
        "  writer.append_data(image)\n",
        "embed.embed_file(gif_file)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}